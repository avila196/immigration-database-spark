{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# IMMIGRATION ANALYSIS IN THE US!\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "The present project takes into account some US datasets regarding the immigration during a fixed amount of time\n",
    "and performs some data pipelines in order to clean the raw dataset and create a data model that serves analytic\n",
    "purposes, specifically the ones to answer the following questions:\n",
    "* What is the state with most immigrants registered?\n",
    "* How many airports have each of the top 10 cities with most immigrants?\n",
    "* What is the state with greater immigrants/population rate?\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Imports needed for the program\n",
    "import os\n",
    "import glob\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, col, monotonically_increasing_id\n",
    "from pyspark.sql.types import DoubleType, IntegerType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "The scope of the project is to read the following datasets in order to satisfy the analytical questions stated above:\n",
    "* immigration_data (SAS format written in PARQUET files)\n",
    "* airport-codes_csv (CSV format)\n",
    "* us-cities-demographics (CSV format)\n",
    "\n",
    "The project gathers these datasets in order to create a star schema considering a main fact table (immigration data) and three dimension tables (immigrant info, airports information and cities demographics). These star schema will be represented using Spark SQL module, using Spark to read all input files (parquet and csv) and to create the needed dataframes.\n",
    "The questions to answer are the following:\n",
    "\n",
    "* What is the state with most immigrants registered?\n",
    "* How many airports have each of the top 10 cities with most immigrants?\n",
    "* What is the state with greater immigrants/population rate?\n",
    "\n",
    "#### Describe and Gather Data\n",
    "\n",
    "* I94 Immigration Data: This data comes from the US National Tourism and Trade Office. A data dictionary is included in the repository. This data consists of records of immigrants on US.\n",
    "* U.S. City Demographic Data: This data comes from OpenSoft. It consists of records of cities in US and all their demographic information such as population (both male and female), main race, number of veterans, among others.\n",
    "* Airport Code Table: This is a simple table of airport codes and corresponding cities. It comes from datahub: https://datahub.io/core/airport-codes#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def getSparkSession():\n",
    "    \"\"\"\n",
    "    Returns a new Spark session object\n",
    "    \n",
    "    Parameters:\n",
    "        None\n",
    "        \n",
    "    Returns:\n",
    "        spark: Spark session\n",
    "    \"\"\"\n",
    "    #Create Spark session\n",
    "    return SparkSession.builder.\\\n",
    "    config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    "    .enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def loadDFs(spark):\n",
    "    \"\"\"\n",
    "    Loads and returns all Dataframes needed using the given spark object\n",
    "    \n",
    "    Parameters:\n",
    "        df: Spark Dataframe\n",
    "        \n",
    "    Returns:\n",
    "        immigration_df: Immigration Dataframe\n",
    "        airports_df:    Airports Dataframe\n",
    "        cities_df:      Cities Dataframe\n",
    "    \"\"\"\n",
    "    #Reading immigration data (from parquet files)\n",
    "    immigration_df = spark.read.parquet(\"sas_data\")\n",
    "\n",
    "    #Reading airport codes (from csv)\n",
    "    airports_df = spark.read.format('csv').options(header='true').load('airport-codes_csv.csv')\n",
    "    airports_df = airports_df.withColumn(\"elevation_ft\", airports_df[\"elevation_ft\"].cast(IntegerType()))\n",
    "\n",
    "    #Reading us cities demographics (from csv)\n",
    "    cities_df = spark.read.format('csv').options(header='true').load('us-cities-demographics.csv')\n",
    "    cities_df = cities_df.toDF(\"data\")\n",
    "    cities_df = cities_df.select(split(col(\"data\"),\";\").getItem(0).alias(\"city\"),\n",
    "        split(col(\"data\"),\";\").getItem(1).alias(\"state\"),\n",
    "        split(col(\"data\"),\";\").getItem(2).alias(\"median_age\").cast(DoubleType()),\n",
    "        split(col(\"data\"),\";\").getItem(3).alias(\"male_pop\").cast(IntegerType()),\n",
    "        split(col(\"data\"),\";\").getItem(4).alias(\"female_pop\").cast(IntegerType()),\n",
    "        split(col(\"data\"),\";\").getItem(5).alias(\"total_pop\").cast(IntegerType()),\n",
    "        split(col(\"data\"),\";\").getItem(6).alias(\"num_veterans\").cast(IntegerType()),\n",
    "        split(col(\"data\"),\";\").getItem(7).alias(\"foreign_born\").cast(IntegerType()),\n",
    "        split(col(\"data\"),\";\").getItem(8).alias(\"avg_household_size\").cast(DoubleType()),\n",
    "        split(col(\"data\"),\";\").getItem(9).alias(\"state_code\"),\n",
    "        split(col(\"data\"),\";\").getItem(10).alias(\"race\"),\n",
    "        split(col(\"data\"),\";\").getItem(11).alias(\"count\").cast(IntegerType())).drop(\"data\")\n",
    "\n",
    "    return immigration_df, airports_df, cities_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Schemas of DF read from files\n",
    "def dispSchema(df, title):\n",
    "    \"\"\"\n",
    "    Prints the schema of the given Spark dataframe\n",
    "    \n",
    "    Parameters:\n",
    "        df: Spark Dataframe\n",
    "        title: Table name\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    print(title,\"Table Schema:\")\n",
    "    df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Given the datasets provided, and looking at the sample CSV files, the main fields to consider in order to answer the analytical queries cannot have NULL values. These entire records will be removed in order to leave the datasets clean to perform the needed queries. Also, duplicate rows are not allowed for specific columns and they will be removed as described next.\n",
    "\n",
    "#### Cleaning Steps\n",
    "The cleaning steps are explained as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def cleanDFs(immigration_df, airports_df, cities_df):\n",
    "    \"\"\"\n",
    "    Cleans the given dataframes according to the rules defined\n",
    "    \n",
    "    Parameters:\n",
    "        immigration_df: Immigration Dataframe\n",
    "        airports_df:    Airports Dataframe\n",
    "        cities_df:      Cities Dataframe\n",
    "        \n",
    "    Returns:\n",
    "        immigration_df: Immigration Dataframe cleaned\n",
    "        airports_df:    Airports Dataframe cleaned\n",
    "        cities_df:      Cities Dataframe cleaned\n",
    "    \"\"\"\n",
    "    #Immigration DF\n",
    "    #This dataframe cannot include NULL values for i94addr, depdate or gender\n",
    "    immigration_df = immigration_df.na.drop(subset=[\"i94addr\",\"depdate\",\"gender\"])\n",
    "\n",
    "    #Airport Codes DF\n",
    "    #This dataframe cannot have duplicates for ident and cannot have NULL values for municipality\n",
    "    airports_df = airports_df.na.drop(subset=[\"municipality\"]).dropDuplicates(subset=[\"ident\"])\n",
    "\n",
    "    #US cities demographics DF\n",
    "    #This dataframe cannot have NULL values for total population and both city-state should be unique for each row\n",
    "    cities_df = cities_df.na.drop(subset=[\"total_pop\"]).dropDuplicates(subset=[\"city\", \"state\"])\n",
    "    \n",
    "    return immigration_df, airports_df, cities_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "The database schema is presented as follows. It consists of 1 fact table with all immigration data needed to perform queries and 3 dimension tables that have all the information needed for the immigrant person, airports and cities demographics.\n",
    "![](db_images/db_immigration.png)\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "In order to perform the data pipeline to model the database needed is as follows:\n",
    "* Create and load immigrants info table from immigration dataframe casting some double columns into integers for better performance\n",
    "* Create and load airports table from airports dataframe, selecting only the US airports\n",
    "* Create and load cities demographics table from cities dataframe, selecting the relevant cities' features\n",
    "* Create and laod the main fact table, immigration table, from the immigration dataframe, casting some double columns into integers for better performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Immigrants Table Schema:\n",
      "root\n",
      " |-- imm_id: integer (nullable = true)\n",
      " |-- birth_year: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- visa_type: string (nullable = true)\n",
      "\n",
      "Airports Table Schema:\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: integer (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n",
      "Cities Demographics Table Schema:\n",
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- state_name: string (nullable = true)\n",
      " |-- male_pop: integer (nullable = true)\n",
      " |-- female_pop: integer (nullable = true)\n",
      " |-- total_pop: integer (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      "\n",
      "Immigration (fact) Table Schema:\n",
      "root\n",
      " |-- imm_id: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- country_code: integer (nullable = true)\n",
      " |-- airport_code: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- departure_date: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Data pipeline to create the fact table and the 3 dimension tables as needed\n",
    "def buildDataModel():\n",
    "    \"\"\"\n",
    "    Builds the data model using the complete pipeline defined\n",
    "    \n",
    "    Parameters:\n",
    "        None\n",
    "        \n",
    "    Returns:\n",
    "        immigrants_info_table:      Immigrants dimension table\n",
    "        airports_table:             Airports dimension table\n",
    "        cities_demographics_table:  Cities dimension table\n",
    "        immigration_table:          Immigration fact table\n",
    "    \"\"\"\n",
    "    \n",
    "    #First, create a Spark Session\n",
    "    spark = getSparkSession()\n",
    "    \n",
    "    #Second, load the DFs and print their schemas\n",
    "    immigration_df, airports_df, cities_df = loadDFs(spark)\n",
    "    \n",
    "    #Third, clean the DFs\n",
    "    immigration_df, airports_df, cities_df = cleanDFs(immigration_df, airports_df, cities_df)\n",
    "    \n",
    "    #Fourth, create tables following the star schema described\n",
    "    #Immigrant info table (dim)\n",
    "    immigrants_info_table = immigration_df.select(col(\"cicid\").cast(IntegerType()).alias(\"imm_id\"), \n",
    "                            col(\"biryear\").cast(IntegerType()).alias(\"birth_year\"), \"gender\", \n",
    "                            col(\"visatype\").alias(\"visa_type\"))\n",
    "\n",
    "    #Airport codes table (dim) (filet to get only US countries)\n",
    "    airports_table = airports_df.filter(\"iso_country == 'US'\").select(col(\"ident\").alias(\"id\"), \"type\", \"name\", \n",
    "                            \"elevation_ft\", split(col(\"iso_region\"), \"-\").getItem(1).alias(\"state\"), \n",
    "                            col(\"municipality\").alias(\"city\"))\n",
    "\n",
    "    #US cities demographics table (dim)\n",
    "    cities_demographics_table = cities_df.select(\"city\", col(\"state\").alias(\"state_name\"), \"male_pop\", \"female_pop\",\n",
    "                            \"total_pop\", col(\"state_code\").alias(\"state\"), \"race\")\n",
    "\n",
    "    #Immigration fact table\n",
    "    immigration_table = immigration_df.select(col(\"cicid\").cast(IntegerType()).alias(\"imm_id\"), \n",
    "                            col(\"i94yr\").cast(IntegerType()).alias(\"year\"), col(\"i94cit\").cast(IntegerType()).alias(\"country_code\"),\n",
    "                            col(\"i94port\").alias(\"airport_code\"), col(\"i94addr\").alias(\"state\"), \n",
    "                            col(\"depdate\").cast(IntegerType()).alias(\"departure_date\"))\n",
    "    \n",
    "    #Fifth, print schemas of tables created\n",
    "    dispSchema(immigrants_info_table, \"Immigrants\")\n",
    "    dispSchema(airports_table, \"Airports\")\n",
    "    dispSchema(cities_demographics_table, \"Cities Demographics\")\n",
    "    dispSchema(immigration_table, \"Immigration (fact)\")\n",
    "    \n",
    "    \n",
    "    return immigrants_info_table, airports_table, cities_demographics_table, immigration_table\n",
    "\n",
    "#Call to build data model\n",
    "immigrants_info_table, airports_table, cities_demographics_table, immigration_table = buildDataModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "In order to ensure the integrity of the tables, the following tests are run:\n",
    " * Immigration fact table has no duplicate entries for same person\n",
    " * Immigrants table has no duplicate entries for same person\n",
    " * Cities demographics table have unique (city,state) pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "#Check that imm_id is unique in both immigrations fact table and immigrants info table\n",
    "if immigrants_info_table.groupBy(\"imm_id\").count().filter(\"count > 1\").count() > 0:\n",
    "    raise ValueError(\"Immigrants table violates restriction of unique IM ID\")\n",
    "if immigration_table.groupBy(\"imm_id\").count().filter(\"count > 1\").count() > 0:\n",
    "    raise ValueError(\"Main Immigration table violates restriction of unique IM ID\")\n",
    "\n",
    "#Check that cities table have unique cities-state composite entries\n",
    "if cities_demographics_table.groupBy(\"city\",\"state\").count().filter(\"count > 1\").count() > 0:\n",
    "    raise ValueError(\"Cities table violates restriction of unique CITY entries\")\n",
    "\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "In order to get the fields description of the database, refer to the `data_dictionary.txt`file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "The project was developed using mainly Spark to read and load all files into dataframes (used as staging elements to start). Spark was considered due to its ability to handle big data (when needed) and to load data into dataframes from PARQUET files. Also, Spark can be integrated with any cloud service where files could be located such as Amazon's S3 or any HDFS path.\n",
    "\n",
    "Regarding the data model, the star schema was used because it provides the perfect adjustment between all datasets available and the main (fact) table for immigration records. These records include specific values that can be easily integrated (aggregated) with the other datasets to perform the analytical queries needed, enabling faster results for the queries considered. Also, the OLAP cubes can be constructed efficiently to serve the purposes described here and further analytical purposes as well.\n",
    "\n",
    "In order for the analysis to remain valid through time, the datasets should be updated once a year to make sure all conclusions are up to date for new immigration data. Given the data model currently in use, the data pipeline can produce several analytical purposes in order to answer questions such as:\n",
    "* What is the state with most immigrants registered?\n",
    "* How many airports have each of the top 10 cities with most immigrants?\n",
    "* What is the state with greater immigrants/population rate?\n",
    "\n",
    "##### Different Possible Scenarios\n",
    " * The data was increased by 100x: Spark should be adjusted to run within a cloud cluster using the 100x files as inputs from services such as S3 or HDFS. This cluster allows the distributed-fashion Spark uses to store data and perform queries on it.\n",
    " \n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day: Data pipelines can be automated using Airflow DAG's scheduled once a day starting at 7am. This DAG should include:\n",
    " 1. Load from datasets available\n",
    " 2. Perform data cleaning on dashboard, deleting current data results\n",
    " 3. Transform data into staging tables\n",
    " 4. Load data models defined to create database schema\n",
    " 5. Perform data quality checks to ensure database integrity\n",
    " \n",
    " * The database needed to be accessed by 100+ people: On one side, to cover an increasing demand, cloud resources should be used to store data models and make them available for lots of queries. On the other hand, more data integrity checks should be made in order to avoid data loss and data rupture of integrity. Copies of the databases can be allocated beforehand to store different states of the main database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
